# -*- coding: utf-8 -*-
"""HR_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16uKwY6OdySqOMBC3UwbibrIx8QCnDf9v

# 1. **Importing & Loading**
"""

import os
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec
from tabulate import tabulate
# Ignore  the warnings
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')
print("Libraries imported successfully!")

# Load the dataset
df = pd.read_csv('olist_orders_merged.csv')

"""# 2. **Quality Checks**

## 2.1 Viewing
"""

def show_unique_values(df):

    # Iterate over each column in the DataFrame
    for column in df.columns:
        # Get unique values for the column
        unique_values = df[column].unique()

        # Create a list to store the results
        results = []

        # Append the column name and its unique values to the results list
        results.append([column, ", ".join(map(str, unique_values))])

        # Print the results in a table format
        print(tabulate(results, headers=["Column", "Unique Values"], tablefmt="pretty"))

show_unique_values(Employee_df)

show_unique_values(PerformanceRating_df)

"""## 2.2 Missing Values & Data Types"""

# Check missing values
Employee_df.info()

# Check zeros in numeric columns
(Employee_df[["YearsWithCurrManager", "Salary", "Age","DistanceFromHome (KM)"]] == 0).sum()

# Check missing values
PerformanceRating_df.info()

"""## 2.3 Descriptive Statistics"""

Employee_df.describe()

PerformanceRating_df.describe()

"""## 2.3 Duplicates"""

# Employee duplicates
Employee_df["EmployeeID"].duplicated().sum()

# Performance duplicates
PerformanceRating_df["PerformanceID"].duplicated().sum()

"""## 2.4 Orphaned Data"""

# Checks for any unmatched IDs in PerformaneRating
PerformanceRating_df[~PerformanceRating_df["EmployeeID"].isin(Employee_df["EmployeeID"])]

"""# 3. **EDA**

## 3.1 Outliers
"""

# Initialize style
sns.set_style("darkgrid")
plt.rcParams['figure.dpi'] = 100


def plot_outliers():
    """Visualize outliers in key numerical features"""
    fig = plt.figure(figsize=(18, 12))
    gs = gridspec.GridSpec(3, 2)

    # Salary Distribution
    ax1 = fig.add_subplot(gs[0, 0])
    sns.boxplot(x=Employee_df['Salary'], color='royalblue')
    ax1.set_title('Salary Distribution', weight='bold')
    ax1.set_xlabel('Annual Salary (USD)')

    # Age Distribution
    ax2 = fig.add_subplot(gs[0, 1])
    sns.boxplot(x=Employee_df['Age'], color='darkorange')
    ax2.set_title('Age Distribution', weight='bold')

    # Tenure Analysis
    ax3 = fig.add_subplot(gs[1, 1])
    sns.boxplot(x=Employee_df['YearsAtCompany'], color='forestgreen')
    ax3.set_title('Company Tenure (Years)', weight='bold')

    # Distance from Home
    ax4 = fig.add_subplot(gs[1, 0])
    sns.boxplot(x=Employee_df['DistanceFromHome (KM)'], color='crimson')
    ax4.set_title('Distance from Home', weight='bold')

    # Salary vs Job Role
    ax7 = fig.add_subplot(gs[2, :])
    sorted_roles = Employee_df.groupby('JobRole')['Salary'].median().sort_values(ascending=False).index
    sns.boxplot(x='JobRole', y='Salary', data=Employee_df, order=sorted_roles, palette='Set3')
    ax7.set_title('Salary Distribution by Job Role', weight='bold')
    ax7.set_xticklabels(ax7.get_xticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

# Run the function
plot_outliers()

# Additional Outlier Checks
def numerical_outlier_analysis(df, columns):
    """Statistical outlier detection"""
    for col in columns:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - (1.5 * iqr)
        upper_bound = q3 + (1.5 * iqr)

        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        print(f"\n{col} outliers ({len(outliers)} cases):")
        print(outliers[['EmployeeID', col]].head())

# Analyze key numerical columns
numerical_columns = ['Salary', 'Age', 'YearsAtCompany', 'DistanceFromHome (KM)']
numerical_outlier_analysis(Employee_df, numerical_columns)

"""## 3.2 Exploratoin"""

# Merge data for analysis
merged_df = pd.merge(Employee_df, PerformanceRating_df, on="EmployeeID", how="left")

# Define columns to visualize
categorical_columns = [
    "Education", "JobSatisfaction", "EnvironmentSatisfaction",
    "RelationshipSatisfaction", "WorkLifeBalance", "Gender",
    "Department", "JobRole"
]
def plot_countplots(df, columns):
    # Initialize style
    sns.set_style("whitegrid")
    plt.rcParams['figure.dpi'] = 100

    """Create countplots for categorical columns"""
    num_plots = len(columns)
    fig, axes = plt.subplots(nrows=(num_plots + 1) // 2, ncols=2, figsize=(18, 6 * ((num_plots + 1) // 2)))
    axes = axes.flatten()  # Flatten for easy iteration

    for i, col in enumerate(columns):
        # Create countplot
        sns.countplot(x=col, data=df, palette="viridis", ax=axes[i])
        axes[i].set_title(f"Distribution of {col}", weight="bold")
        axes[i].set_xlabel(col)
        axes[i].set_ylabel("Count")

        # Add percentage annotations
        total = len(df[col].dropna())
        for p in axes[i].patches:
            height = p.get_height()
            axes[i].text(
                p.get_x() + p.get_width() / 2., height + 3,
                f"{height/total:.1%}", ha="center"
            )

        # Rotate x-labels if needed
        if len(df[col].unique()) > 5:
            axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha="right")

    # Remove empty subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

# Run the function
plot_countplots(merged_df, categorical_columns)

"""# 4. **Cleaning**

## 4.1 Dropping Columns
"""

# Columns irrelevant to analysis (e.g., names, redundant IDs)
Employee_df.drop(["FirstName", "LastName", "State"], axis=1, inplace=True)

"""## 4.2 Fixing a Typo"""

Employee_df["BusinessTravel"] = Employee_df["BusinessTravel"].str.strip()  # Remove trailing spaces

"""## 4.2 Aligning Dates"""

# Convert ReviewDate and HireDate to US format
Employee_df["HireDate"] = pd.to_datetime(Employee_df["HireDate"]).dt.strftime('%m/%d/%Y')
PerformanceRating_df["ReviewDate"] = pd.to_datetime(PerformanceRating_df["ReviewDate"]).dt.strftime('%m/%d/%Y')

"""## 4.3 Validation"""

Employee_df

PerformanceRating_df

"""## 4.4 Saving"""

# Save cleaned data
Employee_df.to_csv("Employee_Cleaned.csv", index=False)
PerformanceRating_df.to_csv("Performance_Cleaned.csv", index=False)

"""# 5. Forecasting

## 5.2 Correlation
"""

for index, row in Employee_df.iterrows():
    if row['Attrition'] == 'Yes':
        Employee_df.loc[index, 'Attrition_group'] = 1
    else:
        Employee_df.loc[index, 'Attrition_group'] = 0

Employee_df['Attrition_group'] = Employee_df['Attrition_group'].astype('int')

Employee_df.drop(["EmployeeID", "HireDate", "Attrition", "OverTime", "MaritalStatus", "EducationField", "JobRole", "Ethnicity", "Department", "Gender", "BusinessTravel", "Gender"], axis=1, inplace=True)

# Correlation Matrix
f, ax = plt.subplots(figsize=(12, 8))
corrmat = Employee_df.corr()
k = 27 # Number of variables for heatmap
cols = corrmat.nlargest(k, 'Attrition_group')['Attrition_group'].index
cm = np.corrcoef(Employee_df[cols].values.T)
sns.set(font_scale=0.75)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 7}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()